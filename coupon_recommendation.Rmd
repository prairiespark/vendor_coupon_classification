---
title: "Predictive Classification of In-Vehicle Coupon Recommendation"
author: "Omer Mejia"
date: "9/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      paged.print = FALSE)
```
  
## A. Project Goals and Approach

### 1. Project Goal and Significance

The goal is to build a preliminary classification model to predict whether the target person will accept a coupon offer given the conditions.

I have particular interest in this type of data (marketing) because it is highly relevant to other promotion related endeavors in the non-profit sector such as fundraising and influencing people to perform positive behaviors.


### 3. Approach 

This study will explore multiple supervised learning algorithms and determine  which among the models developed best predicts the behavior or choice of the person.

To prepare and conduct the classification prediction model, this project will utilize the `tidverse` and `tidymodels` packages in R. 

The `tidymodels` package provides a systematic and unified interface in conducting machine learning models using various algorithms. The machine learning algorithms that will be used will be discussed later in the modeling section.




### 4. Load packages

This project will use the `tidyverse` and `tidymodels` packages.

```{r}

library(tidyverse) # data analysis and modeling
library(tidymodels) # data analysis and modeling
library(naniar) # analysis of missing data
library(DataExplorer) # analysis of missing data
library(visdat) # analysis of missing data


```

## B. Data Exploration and Cleaning

### 1. Dataset Information

The dataset was downloaded from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/in-vehicle+coupon+recommendation). It was uploaded on the website on September 15, 2020.

Tong Wang (University of Iowa) and Cynthia Rudin (Duke University) published a paper in 2017 that used this data. They shared the data to UCI ML Repository.  

From the dataset information in the UCI: "This data was collected via a survey on Amazon Mechanical Turk. The survey describes different driving scenarios including the destination, current time, weather, passenger, etc., and then ask the person whether he will accept the coupon if he is the driver."

Data Set Characteristics:  Multivariate
Number of Instances: 12684
Area: Business
Attribute Characteristics: N/A
Number of Attributes: 23
Date Donated: 2020-09-15
Missing Values? Yes



### 2. Data Ingestion 

The file is in a CSV format. The data will be imported as `accept_coupon` using `read.csv`

```{r}
accept_coupon <- read.csv("in-vehicle-coupon-recommendation.csv")
```

```{r}
str(accept_coupon)
```

	
The attribute information can be found in the dataset webpage <https://archive.ics.uci.edu/ml/datasets/in-vehicle+coupon+recommendation>

The outcome variable is `Y` which indicates `1` (accepted the coupon) and `0` (not accepted the coupon)


### 3. Data Exploration

First, we will explore the data using the built in R `glimpse()` and `summary()` functions. 

```{r}
glimpse(accept_coupon)
```

```{r}
accept_coupon %>% 
  summary()
```


Further, we will employ exploration tools from othe packages. The `introduce()` and `plot_intro()` functions from the `DataExplorer` package provides an overview and visualization of the data. Finally, produce a data exploration report using `DataExplorer`


```{r}
accept_coupon %>% 
  DataExplorer::introduce()
```

```{r}
accept_coupon %>% 
  DataExplorer::plot_intro()
```

```{r eval=FALSE}
accept_coupon %>% 
  DataExplorer::create_report()
```






Investigating the `income` variable, we can see that income are divided into ranges such that it can be considered as categorical values. However, later it must be transformed into factors with the correct order of categories.

```{r}
accept_coupon %>% 
  select(income) %>% 
  unique()
```



Check for multicollinearity using `cor()` and visualize it using `corrplot()` of the `corrplot` package

```{r}
accept_coupon %>% 
  select_if(is.numeric) %>%
  cor() %>% 
  corrplot::corrplot(method = "ellipse")
```

The direction_same and direction_opp is exactly the same. This will cause an error in the modeling process. We can safely remove either one of them. 

### 3. Data Cleaning

Looking again at the data, there are entries which have the value `""`.

Counting the number of `""` in the dataset, there are about 13,370 entries with "" values. These are supposed to be NAs.  

```{r}
length(grep("", accept_coupon))
```

```{r}
length(accept_coupon[accept_coupon == ""])
```

Converting the `""` into `NA` and checking again the number of `NA`s in the dataset, we find the following:

```{r}
accept_coupon[accept_coupon == ""] <- NA

```

```{r}
accept_coupon %>% 
  introduce()
```
Visualizing the missng data can give us a better understanding of the missingness.

```{r}
accept_coupon %>% 
  vis_miss()
```

Using the `vis_dat` function allows us to visualize whether the missing values are coming from categorical (character) variables or numerical variables. Below, we can see that there

```{r}
accept_coupon %>% 
  vis_dat()
```


```{r}
accept_coupon %>% 
  naniar::miss_var_summary()
```

Almost all of the values of the `car` variable is missing! Checking the nature of the variables, the entries do not have a meaningful pattern that can be significant in influencing the model. We can safely exclude the whole variable in the modeling process.

```{r}
unique(accept_coupon$car)
```

In the process of cleaning the dataset, it will be renamed as `coupon`. This will allow us to revisit the original data if there will be additional inquiries that we would like to perform.






Now we will remove the `car` variable (to eliminate missing values) and `direction_opp` variables (to remove multicollinearity)

```{r}
# removing the `car` variable
coupon <- accept_coupon %>% 
  select(-c(car,direction_opp))
```


```{r}
setdiff(colnames(accept_coupon), colnames(coupon))
```


```{r}
miss_var_summary(coupon)
```


```{r}
coupon %>% 
  mutate(
    across(where(is.character), as_factor)
  ) %>% 
  str()
  
```

The percentage of the missing values in the variables `CoffeeHouse`,  `CarryAway`, `Bar`, `Restaurant20To50`, and `Restaurant20To50` are at the most 1.8 percent of a variable. We can also safely drop the rows with `NA`s without significant effect on the model   

```{r}
dim(coupon)
```


```{r}
coupon <- coupon %>% 
  drop_na()
```

```{r}
12079 / 12684
```

```{r}
coupon %>% 
  vis_miss()
```

```{r}
which(coupon == "", arr.ind = TRUE)
```


More than 95% of the data is retained.

A summary of the numeric values of the dataset shows the following statistics. The outcome value `Y` shows that the proportion of "Yes" (`1`) values is 0.57. This is a good proportion of outcome values where one outcome is not predominant over the other which will lead to a uneven distribution of outcomes in training and testing data. 

```{r}
coupon %>% 
  select_if(is.numeric) %>% 
  summary()
```


Now we will convert the income into a factor with spectific order

```{r}
# Extracting the categories

coupon %>% 
  select(income) %>% 
  unique() %>% 
  pull()
```

```{r}
# Converting into an ordered factor

coupon <- coupon %>% 
  mutate(income = factor(income,
            levels = c(
              "Less than $12500",
              "$12500 - $24999",
              "$25000 - $37499",
              "$37500 - $49999",
              "$50000 - $62499",
              "$62500 - $74999",
              "$75000 - $87499",
              "$87500 - $99999",
              "$100000 or More"
            )))
```

```{r}
# Converting into an ordered factor

levels(coupon$income) <- c("A", "B", "C", "D", "E", "F", "G", "H", "I")

levels(coupon$income)
```





Converting `Y` into a factor variable

```{r}
coupon$Y <- factor(coupon$Y)
```


Check if there is a class imbalance for the outcome variable `Y`

```{r}
coupon %>% 
  count(Y)
```

Though the class imbalance is not great, a downsampling will be performed to improve modeling performance


```{r paged.print=FALSE}
coupon %>% 
  DataExplorer::plot_bar()
```

```{r}
?DataExplorer::create_report
```



```{r eval=FALSE}
coupon %>% 
  DataExplorer::create_report(output_file = "cleaned_report.html")
```






## C. Resampling and Data Preprocessing 

### 1. Splitting into training and test sets

We will use the `tidymodels` packages to conduct resampling and data preprocessing. 

To ensure reproducibility, the seed of the random number generator will be set to 45 and the dataset will be split into training and test sets using `initial_split`, `training`, and `testing` functions from the `rsample` package. 

```{r}
# Create the split
# proportion of training is 70%; strata = Y is the outcome variable that will be predicted 

set.seed(45)

coupon_split <- rsample::initial_split(coupon, # cleaned dataset
                                       prop = 0.7, # proportion of training data
                                       strata = Y) # outcome variable
```

```{r}
coupon_training <- coupon_split %>% 
  rsample::training()

coupon_test <- coupon_split %>% 
  rsample::testing()
```


### 2.  Creating cross-validation folds

K-cross validation folds will be developed to help validate the best model.

The are `r nrow(accept_coupon)` instances for cleaned data set (after removing missing data). A 5-fold cross validation with approximately `r round(nrow(accept_coupon)/5)` instances for each fold is reasonable.

Using the `vfold_cv` function of `rsample`

```{r}
set.seed(45)
coupon_folds <- rsample::vfold_cv(coupon_training, # training dataset
                                  v = 5, # number of folds
                                  strata = Y) # outcome variable
```

The result is a nested tibble (a special data frame that allows list entries). 

```{r}
coupon_folds
```

### 3. Feature engineering

Next, we will conduct feature engineering using `recipes` package to transform feature variables suitable for machine learning.

The following steps will be performed to build the data preprocessing recipe:
1. Scale numerical values
2. Convert character features into factors
3. Convert factors into dummy variables

```{r}
# library(themis)
```


```{r}
set.seed(45)
coupon_recipe <- recipe(Y ~ .,
                        data = coupon_training) %>% 
  # Downsampling
  themis::step_downsample(Y) %>% 
  # Scale numerical variables
  #step_normalize(all_numeric(), -all_outcomes()) %>% 
  # Convert character features into factors
  step_novel(all_nominal(), -all_outcomes()) %>% 
  # Convert factors into dummy variables 
  step_dummy(all_nominal(), -all_outcomes())
  
```

```{r}
coupon_recipe
```


## D. Specifying models and the workflow

In this classification problem we will use the following algorithms in R tha are accessible in the `tidymodels` interface:

* Logistic regression
* Decision trees
* Random forests
* Boosted trees



### 1. Specifying the models


Specify logistic regression - glm engine

```{r}
spec_log_reg_glm <- logistic_reg() %>% 
  set_engine(engine = "glm") %>% 
  set_mode("classification")

spec_log_reg_glm
```


Specify decision trees model

```{r}
library(rpart)

spec_dt <- decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

spec_dt
```


Specify random forest model

```{r}
library(ranger)

spec_rf <- rand_forest() %>% 
  set_engine(engine = "ranger", importance = "impurity") %>% 
  set_mode("classification")

spec_rf
```


Specify boosted tree model

```{r}
library(xgboost)

spec_xgboost <- boost_tree() %>% 
  set_engine(engine = "xgboost") %>% 
  set_mode("classification")

spec_xgboost
```







### 2. Initialize the workflows and choose model


To streamline the model process, the `workflow()` function from the `workflows` package will be used to combine the specified models and the data preprocessing recipe.

```{r}
library(workflows)
```

```{r}
?`workflows-package`
```


logistic regression (glm engine) workflow

```{r}
glm_workflow <- workflow() %>% 
  add_model(spec_log_reg_glm) %>% 
  add_recipe(recipe = coupon_recipe)

glm_workflow
```

```{r}
glm_metrics <- glm_workflow %>% 
  fit_resamples(coupon_folds) %>% 
  collect_metrics()

glm_metrics
```



decision tree workflow

```{r}
dt_workflow <- workflow() %>% 
  add_model(spec_dt) %>% 
  add_recipe(recipe = coupon_recipe)

dt_workflow
```

```{r}
?tune::fit_resamples
```



```{r}
dt_metrics <- dt_workflow %>% 
  fit_resamples(coupon_folds) %>% 
  collect_metrics()

dt_metrics
```


```{r}
rf_workflow <- workflow() %>% 
  add_model(spec_rf) %>% 
  add_recipe(recipe = coupon_recipe)

rf_workflow
```



```{r}
rf_metrics <- rf_workflow %>% 
  fit_resamples(coupon_folds) %>% 
  collect_metrics()

rf_metrics
```


```{r}
xg_workflow <- workflow() %>% 
  add_model(spec_xgboost) %>% 
  add_recipe(recipe = coupon_recipe)

xg_workflow
```

```{r}
xg_metrics <- xg_workflow %>% 
  fit_resamples(coupon_folds) %>% 
  collect_metrics()

xg_metrics
```




```{r}
all_metrics <- bind_rows("log_regression" = glm_metrics,
                     "decision_trees" = dt_metrics,
                     "random_forests" = rf_metrics,
                     "xg_boost" = xg_metrics,
                     .id = "models")

all_metrics
```

```{r}
all_metrics %>% 
  filter(.metric == "roc_auc") %>% 
  slice_max(mean)
```

```{r eval=FALSE}
all_metrics %>% 
  filter(.metric == "roc_auc") %>% 
  ggplot(aes(mean))
```



## E. Tuning the hyperparameters


### 1. Identify the hyperparameters of the random forest model

First we identify the hyperparameters that need to be tuned

The arguments for the `rand_forest()` interface are the following:

```{r}
?parsnip::rand_forest
```

* `mtry`: The number of predictors that will be randomly sampled at each split when creating the tree models.

* `trees`: The number of trees contained in the ensemble.

* `min_n`: The minimum number of data points in a node that are required for the node to be split further.


We will specify a random forest model with the parameters set to `tune()`. `tune()` is a placeholder function for the argument values that are to be tuned.

```{r}
spec_rf_tune <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

spec_rf_tune
```


To know more about the parameters that will be tuned, we will pass the specified model to the `parameters()` function of the `dials` package

```{r}
?dials::parameters
```


```{r}
dials::parameters(spec_rf_tune)
```

We can see that the `mtry` parameters needs finalization. To be able to generate values for `mtry` in the tuning grid, the upper bound must be determined, which is data dependent. 

We pass the predictors of the dataset and `mtry()` to the `finalize()` function from the `dials` package. It will return the upper bound for the 

```{r}
?dials::finalize
```

```{r}
coupon_pred <- coupon %>% 
  select(-Y)

dials::finalize(mtry(), coupon_pred)
```

The value 23 will be used as the upper bound for the `mtry` parameter in creating a random grid,




### 2. Creating the tuning grid

After identifying and setting up the parameters for tuning, a grid of tuning parameters will be created usng the `grid_random()` function of the `dials` package.

The `grid_random()` will generate a number values for the tuning parameters randomly. Aside from reduced time in tuning the hyperparameters as compared to a systematic regular grid, randomly selecting the tuning parameters can improve chance of identifying the  values for the hyperparameters that will give the optimal results for the target metrics.


```{r}
# Create grids of tuning parameters

# Random and regular grids can be created for any number of parameter objects.

?dials::grid_random
```

`size`	
A single integer for the total number of parameter value combinations returned for the random grid. If duplicate combinations are generated from this size, the smaller, unique set is returned.

For the size we will use size = 5, as this is for demonstration purposes. Using the `grid_random()` function and passing a range of values for the `mtry` parameter, setting the upper bound value to 23, which was determined earlier using `finalize()`.

```{r}

rf_grid <- grid_random(mtry(range = c(1, 23)),
                        trees(),
                        min_n(),
                        size = 5)
```

```{r}
glimpse(rf_grid)
```


### 3. Creating the tuning workflow and tuning the hyperparameters

To tune the hyperparameters, a workflow will also be created using the specified random forest model with parameters for tuning and the `coupon_recipe`.   

```{r}
rf_tune_wf <- workflow() %>% 
  add_recipe(coupon_recipe) %>% 
  add_model(spec_rf_tune)

rf_tune_wf
```

We can now tune the hyperparameters using `tune_grid()`. For reproducibility, we will also set the random seed to 45.

The tuning duration for the random forest model using the 5 sets (rows) of hyperparameter values will be measured using the `tictoc` package. Knowing the duration of the tuning can provide a rough estimate on the length of time it would take if we will increase the value of `size` argument of the `grid_random()` function to generate a tuning grid. 


```{r}
set.seed(45)

tictoc::tic()

rf_coupon_results <- tune_grid(
    rf_tune_wf,
    resamples = coupon_folds,
    grid = rf_grid
)

tictoc::toc()

```


## F. Finalizing the model and Evaluating with the Unseen Testing Data 


### 1. Selecting the best model  

The results of the hyperparameter tuning is placed is stored in `rf_coupon_results`. Using the `collect_metrics()` we will extract the metrics to evaluate the best set of hyperparameter values that will provide the best value for the target metrics.

```{r}

rf_coupon_results %>% 
  collect_metrics()

```


```{r}
rf_coupon_results %>% 
  collect_metrics(summarize = FALSE)
```


We will use the `roc_auc` metric as the criterion for selecting the set of hyperparameter values.

First, we will display the minimum, median, and maximum values of the `roc_auc` for each set of hyperparameter combinations to show the variation and spread of the values.

```{r}
rf_coupon_results %>% 
  collect_metrics(summarize = FALSE) %>% 
  filter(.metric == "roc_auc") %>% 
  group_by(id) %>% 
  summarize(min_roc_auc = min(.estimate),
            median_roc_auc = median(.estimate),
            max_roc_auc = max(.estimate))
```


To view which one is the best model, the `show_best()` function is used. It will return a tibble with the sorted values starting from best results.  


```{r}
# show_best {tune} - Investigate best tuning parameters

?tune::show_best
```


```{r}
rf_coupon_results %>% 
  show_best(metric = 'roc_auc', n = 5)
```


The best model will be selected using `select_best` function and stored to a variable `best_rf_coupon_model`.

```{r}
best_rf_coupon_model <- rf_coupon_results %>% 
  select_best(metric = 'roc_auc')

best_rf_coupon_model
```


### 2. Finalizing the model workflow

The `best_rf_coupon_model`, which is a set of hyperparameters in the tuning grid that gave the highest `roc_auc` value, will be used to update the final model workflow. 

Using the `finalize_workflow` function we will finalize the random forest workflow:

```{r}
final_coupon_wf <- rf_tune_wf %>% 
  finalize_workflow(best_rf_coupon_model)

final_coupon_wf
```


### 3. Evaluating the model using the testing data


Finally, the tuned random forest model will be fit to the unseen testing dataset. This will be done through the `last_fit()` function, which also passes the split dataset containing both training and testing data.

```{r}
coupon_final_fit <- final_coupon_wf %>% 
  last_fit(split = coupon_split)
```

```{r}
coupon_final_fit
```

```{r}
coupon_final_fit %>% 
  collect_metrics()
```

Compared to the `roc_auc` value of 0.809 of the model using the training data, the `roc_auc` value of 0.829 of the testing data is even better!

















